{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this, we compare our own algo's accuracy with the sciki-learn's algo \n",
    "### We also find out the accuracy's of each, and we intro the confidence of our algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.8\n",
      "0.6\n",
      "0.6\n",
      "0.6\n",
      "0.8\n",
      "Accuracy: 0.978494623655914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "#defining our own main K Nearest Algo\n",
    "def k_nearest_neighbors(data, predict, k=3):\n",
    "    if len(data) >= k :\n",
    "        warnings.warn('K is set to a value less than total voting groups! (YOU R DUMB ACTUALLY)' )       \n",
    "    distances = []\n",
    "    for group in data:\n",
    "        for features in data[group]:\n",
    "            euclidean_distance = np.linalg.norm(np.array(features) - np.array(predict))\n",
    "            #THIS function from numpy is MUCH faster and NOT hard-coded than the normal one.\n",
    "\n",
    "            distances.append([euclidean_distance, group])\n",
    "            #HERE we append to make a list of lists, each list containg distance and its group-number, so we can differentiate\n",
    "            \n",
    "    votes = [i[1] for i in sorted(distances)[:k]]        #can be written as a multi-line for loop, here we are getting the votes and storing them\n",
    "    vote_result = Counter(votes).most_common(1)[0][0]    #counting the votes of each class and finding the most valued class\n",
    "    confidence = Counter(votes).most_common(1)[0][1] / k #confidence is the percentage of corrct-votes out of total vote\n",
    "    return vote_result, confidence\n",
    "\n",
    "\n",
    "df = pd.read_csv('breast-cancer-wisconsin.data.txt')\n",
    "df.replace('?', -99999, inplace=True)\n",
    "df.drop(['id'], 1, inplace=True)\n",
    "\n",
    "full_data = df.astype(float).values.tolist() \n",
    "random.shuffle(full_data)\n",
    "\n",
    "test_size = 0.4                                         \n",
    "train_set = {2:[], 4:[]}                                 \n",
    "test_set = {2:[], 4:[]}\n",
    "train_data = full_data[:-int(test_size*len(full_data))]\n",
    "test_data = full_data[-int(test_size*len(full_data)):]\n",
    "\n",
    "for i in train_data:                                        \n",
    "    train_set[i[-1]].append(i[:-1])\n",
    "for i in test_data:                                    \n",
    "    test_set[i[-1]].append(i[:-1])\n",
    "    \n",
    "total = 0\n",
    "correct = 0\n",
    "for group in test_set:\n",
    "    for data in test_set[group]:\n",
    "        vote, confidence = k_nearest_neighbors(train_set, data, k=5)\n",
    "        if group == vote: correct += 1\n",
    "        else: print (confidence)\n",
    "        total += 1\n",
    "        \n",
    "print ('Accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have printed out the confidence of only those that were incorectly classfied.\n",
    "### Increasing test_size to 40% reduces the 100%-confident-incorrect votes -->> VERY IMP -->> see video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9677419354838708\n"
     ]
    }
   ],
   "source": [
    "#HERE we compare the algos.....in the video.....We did the same BIG for loop on the sklearn algo also\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "#defining our own main K Nearest Algo\n",
    "def k_nearest_neighbors(data, predict, k=3):\n",
    "    if len(data) >= k :\n",
    "        warnings.warn('K is set to a value less than total voting groups! (YOU R DUMB ACTUALLY)' )       \n",
    "    distances = []\n",
    "    for group in data:\n",
    "        for features in data[group]:\n",
    "            euclidean_distance = np.linalg.norm(np.array(features) - np.array(predict))\n",
    "            #THIS function from numpy is MUCH faster and NOT hard-coded than the normal one.\n",
    "\n",
    "            distances.append([euclidean_distance, group])\n",
    "            #HERE we append to make a list of lists, each list containg distance and its group-number, so we can differentiate\n",
    "            \n",
    "    votes = [i[1] for i in sorted(distances)[:k]]        #can be written as a multi-line for loop, here we are getting the votes and storing them\n",
    "    vote_result = Counter(votes).most_common(1)[0][0]    #counting the votes of each class and finding the most valued class\n",
    "    confidence = Counter(votes).most_common(1)[0][1] / k #confidence is the percentage of corrct-votes out of total vote\n",
    "    return vote_result, confidence\n",
    "\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "    df = pd.read_csv('breast-cancer-wisconsin.data.txt')\n",
    "    df.replace('?', -99999, inplace=True)\n",
    "    df.drop(['id'], 1, inplace=True)\n",
    "\n",
    "    full_data = df.astype(float).values.tolist() \n",
    "    random.shuffle(full_data)\n",
    "\n",
    "    test_size = 0.4                                         \n",
    "    train_set = {2:[], 4:[]}                                 \n",
    "    test_set = {2:[], 4:[]}\n",
    "    train_data = full_data[:-int(test_size*len(full_data))]\n",
    "    test_data = full_data[-int(test_size*len(full_data)):]\n",
    "\n",
    "    for i in train_data:                                        \n",
    "        train_set[i[-1]].append(i[:-1])\n",
    "    for i in test_data:                                    \n",
    "        test_set[i[-1]].append(i[:-1])\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for group in test_set:\n",
    "        for data in test_set[group]:\n",
    "            vote, confidence = k_nearest_neighbors(train_set, data, k=5)\n",
    "            if group == vote: correct += 1\n",
    "            total += 1\n",
    "    accuracies.append(correct / total)\n",
    "\n",
    "print (sum(accuracies) / len(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN algo of sciki-learn has some defualt parameters, so it is MUCH faster. See more in copy and video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
